# 7. AI Integration

The Discovery Engine is not just a repository of knowledge — it is an **active reasoning environment**. At the heart of this environment are **AI agents** that operate directly on the CNM (Conceptual Nexus Model), using structured knowledge representations to support synthesis, discovery, and hypothesis generation.

---

## 7.1 Role of AI in the System

At the core of the Discovery Engine is a fundamental shift: AI is not just an end-user tool, but an active participant in knowledge construction. Rather than relying solely on humans to query or synthesise information, the system empowers specialised AI agents to operate directly on structured representations of science — identifying patterns, proposing hypotheses, navigating conceptual space, and even co-designing new ideas.

These agents are not general-purpose chatbots. They are task-specific, graph- and tensor-literate agents that reason over a rich, semantic substrate. Their role is to amplify human cognition by doing what humans cannot: operating over massive, multidimensional knowledge structures at scale and speed.

AI agents serve as:

| Role                        | Description                                                                    |
|-----------------------------|--------------------------------------------------------------------------------|
| **Navigators**              | Traverse and explore the knowledge graph to identify patterns and clusters     |
| **Analysts**                | Perform link prediction, conflict detection, and statistical summarisation     |
| **Designers**               | Generate new structured hypotheses or conceptual artifacts                     |
| **Assistants**              | Respond to user queries, surface related concepts, and recommend experiments   |

These agents are not generic LLMs generating text in isolation — they are **grounded** in a structured, provenance-rich knowledge substrate.

---

## 7.2 Agent Input Contexts

AI agents in the DE system do not work from raw text. Instead, they operate on well-structured, evidence-linked semantic contexts — including slices of the Conceptual Nexus Model (CNM), high-dimensional tensor projections (TCNM), vector spaces for analogy, and provenance chains for source tracking.

Each of these representations gives the agent a different “lens” through which to perceive scientific knowledge: the graph offers explicit logic, the tensor offers algebraic structure, and embeddings offer intuitive similarity. By selecting the right view for the right task, agents can reason with both precision and flexibility.

Each agent interacts with one or more of the following structured views:

| Input Context              | Description                                                                 |
|----------------------------|-----------------------------------------------------------------------------|
| **CNM Subgraph**           | A slice of the knowledge graph relevant to a concept or method              |
| **Tensor Projection (TCNM)** | Encodes high-dimensional semantic relationships among artifacts             |
| **Vector Embedding Space** | For soft similarity, analogy, and clustering tasks                         |
| **Provenance Chains**      | Linked document segments and metadata for validation and traceability       |

Agents choose the representation that best supports the reasoning task at hand.

---

## 7.3 Agent Operations

The true power of the agent layer lies in its diversity of operations. Agents are not only passive observers or filters — they are **semantic actors** that manipulate, extend, and interpret the knowledge graph.

From traversing causal chains and detecting contradictions to performing analogy-driven retrieval or tensor-based inference, agents can engage in sophisticated tasks that approximate — and in some cases exceed — human conceptual navigation. Some act like detectives, others like designers. Together, they constitute a **toolkit for structured scientific reasoning at scale**.

### 1. **Graph-Based Inference**

- Walk the CNM to:
  - Identify transitive causal chains
  - Detect centrality or bridge nodes
  - Aggregate all supporting evidence for a claim

### 2. **Tensor Algebra & Completion**

- Operate on TCNM to:
  - Infer likely missing relationships (tensor completion)
  - Generate latent semantic associations (via contraction or projection)
  - Detect inconsistencies in relational structure

### 3. **Vector Search & Analogy**

- Retrieve nearest-neighbours to:
  - Find analogues in other domains
  - Group similar mechanisms or parameter sets
  - Detect “semantic twins” — different names for similar ideas

### 4. **Knowledge Gap Detection**

- Use graph sparsity + tensor slicing to:
  - Identify unconnected but structurally adjacent nodes
  - Flag missing parameter values or undefined mechanisms
  - Suggest areas for experiment or theory generation

### 5. **Generative Hypothesis Design**

- Compose new artifacts from:
  - Analogies to existing patterns
  - Constraint satisfaction over parameters
  - Formal templates (e.g. "If X causes Y and Z is similar to X…")

---

## 7.4 Agent Capabilities Matrix

Not all agents operate in the same way, and not all reasoning tasks require the same substrate. The table below maps key agent capabilities to the types of data structures they rely on. This helps determine **which representations are most appropriate for which cognitive tasks**, and clarifies how different parts of the CNM infrastructure support distinct reasoning strategies.

It also reinforces a key design insight: **multi-representational reasoning** (i.e., using graph + tensor + embeddings in tandem) is often the most powerful approach.

| Capability                | Graph | Tensor | Embedding | Provenance |
|---------------------------|-------|--------|-----------|------------|
| Concept Expansion         | ✅    | ✅     | ✅        | ❌         |
| Analogy Detection         | ❌    | ✅     | ✅        | ❌         |
| Contradiction Flagging    | ✅    | ✅     | ❌        | ✅         |
| Hypothesis Generation     | ✅    | ✅     | ✅        | ❌         |
| Confidence Estimation     | ✅    | ❌     | ❌        | ✅         |
| Constraint Resolution     | ✅    | ✅     | ❌        | ❌         |
| Multi-hop Reasoning       | ✅    | ❌     | ❌        | ❌         |

---

## 7.5 Agent Types

To make this infrastructure usable and extensible, the DE defines a core set of modular agent types. Each one is tailored to a specific family of tasks — exploration, synthesis, gap detection, contradiction analysis, design, and verification.

These are not black boxes: each agent has a well-defined scope, input type, reasoning logic, and output format. They can be composed into **multi-agent workflows**, called pipelines, that handle complex end-to-end tasks like exploratory review, rapid hypothesis generation, or design verification. This modularity ensures that new domains or capabilities can be added without re-engineering the entire system.

| Agent Name         | Function Description                                                         |
|--------------------|------------------------------------------------------------------------------|
| **Navigator**       | Explores CNM structure to surface relevant concepts and papers              |
| **Synthesiser**     | Aggregates multiple artifacts into a higher-level hypothesis or design       |
| **GapFinder**       | Identifies missing links or unexplored connections                          |
| **Contradictor**    | Flags claims that are in tension or opposition with others                   |
| **Designer**        | Creates new structured artifacts (hypotheses, methods, parameter models)     |
| **Verifier**        | Checks coherence, completeness, and provenance for a given artifact cluster  |

These agents can be composed into **multi-agent workflows** for advanced tasks like literature review automation or research roadmap synthesis.

---

## 7.6 Example Use Cases

To ground the architecture in real-world utility, this section provides concrete scenarios in which agents are deployed to **accelerate research tasks, reveal insights, or scaffold design**. From parameter lookup to cross-domain analogy discovery, these use cases demonstrate how the Discovery Engine shifts knowledge work from a manual, time-intensive process to an agent-augmented, interactive experience.

They also show the **human-AI collaboration loop** in action: agents surface possibilities, and researchers apply judgment, intuition, and design goals to choose what to pursue.

### Use Case 1: Parameter Inference

> Task: "What parameter values are typically used for photothermal activation in soft robotics?"

- Agent performs:
  - Graph walk on `MethodNode → ParameterNode`
  - Embedding clustering for equivalent mechanisms
  - Statistical summarisation of parameter ranges

---

### Use Case 2: Hypothesis Suggestion

> Task: "Propose a hypothesis for intelligent behaviour in material X"

- Agent pipeline:
  - GapFinder identifies missing causal edges for material X
  - Synthesiser composes plausible links from similar materials
  - Designer generates a hypothesis artifact
  - Verifier checks for provenance similarity and novelty

---

## 7.7 Limitations and Mitigations

Like any complex system, the agent layer is not immune to flaws. AI-generated insights may be misleading, incomplete, or misaligned with human interpretation. However, these risks are **anticipated and addressed through design choices** — including source-linked reasoning, transparency layers, feedback channels, and alignment protocols.

This section highlights where agent logic might fall short, and how the system has been engineered to contain and correct those shortcomings. It’s not about perfection — it’s about **bounded trust and recoverable failure**.

| Limitation                     | Mitigation Strategy                                                 |
|--------------------------------|----------------------------------------------------------------------|
| Spurious analogies             | Constraint filtering + evidence weighting                           |
| Overfitting to local structure | Include global tensor context + provenance checks                   |
| Semantic drift                 | Schema-based validation + embedding retraining                      |
| Agent opacity                  | All agent outputs include explanation chain + source paths          |

---

## 7.8 Towards Responsive Agents

The long-term vision for DE agents extends beyond automation. These agents are designed to become **increasingly responsive and self-improving** — not only generating outputs, but monitoring the quality, coherence, and consequences of their own reasoning.

In future iterations, agents will be able to flag their own uncertainty, request schema changes, challenge prior assumptions, and collaborate with other agents in an emergent design dialogue. The goal is a form of **computational epistemology** — where knowledge systems don’t just retrieve what’s known, but actively participate in understanding what is knowable.

Future versions of agents will be able to:

- Assess the reliability of their own outputs
- Request refinement of the underlying templates or schemas
- Compose new types of questions or data structures to better represent unknowns

This enables the Discovery Engine to act as a **co-researcher**, not just a tool.

---

> ☑️ Visual prompt for napkin.ai:
>
> **Title**: “AI Agent Layer in the Discovery Engine”
>
> - Central block: “AI Agents” 🤖
> - Arrows from three inputs:
>     - “CNM Graph View” 🌐
>     - “TCNM Tensor” 🧮
>     - “Embedding Space” 🌌
>
> - Agents shown as modular units:
>     - Navigator 🧭
>     - Synthesiser 🧠
>     - GapFinder 🔍
>     - Contradictor ⚖️
>     - Designer 🧬
>     - Verifier ✅
>
> - Outputs:
>     - New Artifacts 📄
>     - Flagged Gaps ❗
>     - Suggested Hypotheses 💡
>     - Ranked Clusters 📊
